{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>OK, does pineapple belong on a pizza? Relax an...</td>\n",
       "      <td>Ah, the age-old culinary conundrum that has di...</td>\n",
       "      <td>As an AI, I don't eat, so I don't have persona...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>How can I get both of them as quick as possibl...</td>\n",
       "      <td>If you want to get both a marriage license and...</td>\n",
       "      <td>In California, here are the general steps to o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the minimal time to get them? 1 day or...</td>\n",
       "      <td>The minimal time to get a marriage license and...</td>\n",
       "      <td>The marriage license in California is issued i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             model_a     model_b  \\\n",
       "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "1  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "2  53567           koala-13b  gpt-4-0613   \n",
       "3  53567           koala-13b  gpt-4-0613   \n",
       "4  53567           koala-13b  gpt-4-0613   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Is it morally right to try to have a certain p...   \n",
       "1  OK, does pineapple belong on a pizza? Relax an...   \n",
       "2  What is the difference between marriage licens...   \n",
       "3  How can I get both of them as quick as possibl...   \n",
       "4  What is the minimal time to get them? 1 day or...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  The question of whether it is morally right to...   \n",
       "1  Ah, the age-old culinary conundrum that has di...   \n",
       "2  A marriage license is a legal document that al...   \n",
       "3  If you want to get both a marriage license and...   \n",
       "4  The minimal time to get a marriage license and...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  As an AI, I don't have personal beliefs or opi...               1   \n",
       "1  As an AI, I don't eat, so I don't have persona...               1   \n",
       "2  A marriage license and a marriage certificate ...               0   \n",
       "3  In California, here are the general steps to o...               0   \n",
       "4  The marriage license in California is issued i...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               0           0  \n",
       "2               1           0  \n",
       "3               1           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "# Function to safely parse the string as a list\n",
    "def parse_list_string(s):\n",
    "    try:\n",
    "        # First try to parse as JSON\n",
    "        return json.loads(s)\n",
    "    except:\n",
    "        try:\n",
    "            # Then try to parse as Python literal\n",
    "            return ast.literal_eval(s)\n",
    "        except:\n",
    "            # If all fails, return as is\n",
    "            return [s]\n",
    "\n",
    "# Create a new dataframe to store the expanded data\n",
    "expanded_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Parse the prompts and responses\n",
    "    prompts = parse_list_string(row['prompt'])\n",
    "    responses_a = parse_list_string(row['response_a'])\n",
    "    responses_b = parse_list_string(row['response_b'])\n",
    "    \n",
    "    # Make sure all lists have the same length\n",
    "    max_len = max(len(prompts), len(responses_a), len(responses_b))\n",
    "    prompts = prompts + [''] * (max_len - len(prompts))\n",
    "    responses_a = responses_a + [''] * (max_len - len(responses_a))\n",
    "    responses_b = responses_b + [''] * (max_len - len(responses_b))\n",
    "    \n",
    "    # Create a new row for each prompt-response pair\n",
    "    for i in range(max_len):\n",
    "        expanded_rows.append({\n",
    "            'id': row['id'],\n",
    "            'model_a': row['model_a'],\n",
    "            'model_b': row['model_b'],\n",
    "            'prompt': prompts[i],\n",
    "            'response_a': responses_a[i],\n",
    "            'response_b': responses_b[i],\n",
    "            'winner_model_a': row['winner_model_a'],\n",
    "            'winner_model_b': row['winner_model_b'],\n",
    "            'winner_tie': row['winner_tie']\n",
    "        })\n",
    "\n",
    "# Create the expanded dataframe\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Display the first few rows of the expanded dataframe\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (71514, 9)\n",
      "After removing ties: (49865, 9)\n",
      "winner_model_a  winner_model_b\n",
      "1               0                 25181\n",
      "0               1                 24684\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "filtered_df = expanded_df[expanded_df['winner_tie'] == 0]\n",
    "print(f\"Original shape: {expanded_df.shape}\")\n",
    "print(f\"After removing ties: {filtered_df.shape}\")\n",
    "print(filtered_df[['winner_model_a', 'winner_model_b']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in prompt: 0\n",
      "NaN values in response_a: 76\n",
      "NaN values in response_b: 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_10408\\2811217388.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['prompt_resp_a'] = filtered_df['prompt'].fillna('').astype(str) + \" \" + filtered_df['response_a'].fillna('').astype(str)\n",
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_10408\\2811217388.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['prompt_resp_b'] = filtered_df['prompt'].fillna('').astype(str) + \" \" + filtered_df['response_b'].fillna('').astype(str)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59dbf80934c4ef8a421fb9b5e3b3db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_resp_b\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_b\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Now encode\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m embeddings_a \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt_resp_a\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m embeddings_b \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_resp_b\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Step 5: Create feature vectors (multiple approaches)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Approach 1: Concatenate embeddings\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 685\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    687\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:752\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m    751\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    437\u001b[0m     key: value\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    440\u001b[0m }\n\u001b[1;32m--> 442\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    685\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m         output_attentions,\n\u001b[0;32m    692\u001b[0m     )\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:553\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    551\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    552\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m--> 553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:2910\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2902\u001b[0m         layer_norm,\n\u001b[0;32m   2903\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2908\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2909\u001b[0m     )\n\u001b[1;32m-> 2910\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 1: Install sentence-transformers if needed\n",
    "# !pip install sentence-transformers\n",
    "\n",
    "# Step 2: Import and load the SBERT model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model, good balance of speed/performance\n",
    "\n",
    "# Step 3: Prepare text pairs\n",
    "# Check for NaN values\n",
    "print(\"NaN values in prompt:\", filtered_df['prompt'].isna().sum())\n",
    "print(\"NaN values in response_a:\", filtered_df['response_a'].isna().sum())\n",
    "print(\"NaN values in response_b:\", filtered_df['response_b'].isna().sum())\n",
    "\n",
    "# Convert all to strings and handle NaN values\n",
    "filtered_df['prompt_resp_a'] = filtered_df['prompt'].fillna('').astype(str) + \" \" + filtered_df['response_a'].fillna('').astype(str)\n",
    "filtered_df['prompt_resp_b'] = filtered_df['prompt'].fillna('').astype(str) + \" \" + filtered_df['response_b'].fillna('').astype(str)\n",
    "\n",
    "# Now encode\n",
    "embeddings_a = model.encode(filtered_df['prompt_resp_a'].tolist(), show_progress_bar=True)\n",
    "embeddings_b = model.encode(filtered_df['prompt_resp_b'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Step 5: Create feature vectors (multiple approaches)\n",
    "# Approach 1: Concatenate embeddings\n",
    "X_concat = np.hstack([embeddings_a, embeddings_b])\n",
    "\n",
    "# Approach 2: Calculate the difference between embeddings\n",
    "X_diff = embeddings_a - embeddings_b\n",
    "\n",
    "# Approach 3: Calculate absolute difference (may capture similarity better)\n",
    "X_abs_diff = np.abs(embeddings_a - embeddings_b)\n",
    "\n",
    "# Choose which approach to use\n",
    "X = X_concat  # or X_diff or X_abs_diff\n",
    "y = filtered_df['winner_model_a']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (626, 768)\n",
      "Testing data shape: (269, 768)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create and train the Random Forest model\n",
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5799\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.62      0.60       139\n",
      "           1       0.57      0.54      0.55       130\n",
      "\n",
      "    accuracy                           0.58       269\n",
      "   macro avg       0.58      0.58      0.58       269\n",
      "weighted avg       0.58      0.58      0.58       269\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 5: Make predictions and evaluate the model\n",
    "# Predict on test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Model B Wins', 'Model A Wins'],\n",
    "            yticklabels=['Model B Wins', 'Model A Wins'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
